{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# FraudFinder - Feature Engineering (streaming)\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/fraudfinder/raw/main/03_feature_engineering_streaming.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/fraudfinder/blob/main/03_feature_engineering_streaming.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/fraudfinder/blob/main/03_feature_engineering_streaming.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "[FraudFinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the FraudFinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3f6a603e433"
   },
   "source": [
    "### Objective\n",
    "\n",
    "As you engineer features for model training, it's important to consider how the features are computed when making predictions with new data. For online predictions, you may have features that can be pre-computed via _batch feature engineering_. You may also features that need to be computed on-the-fly via _streaming-based feature engineering_. For these Fraudfinder labs, for computing features based on the last n _days_, you will use _batch_ feature engineering in BigQuery; for computing features based on the last n _minutes_, you will use _streaming-based_ feature engineering using Dataflow.\n",
    "\n",
    "In order to calculate very recent customer and terminal activity (i.e. within the last hour), computation has to be done on real-time streaming data, rather than via batch-based feature engineering. This notebook shows a step-by-step guide to create real-time data pipelines to build features. You will learn to:\n",
    "\n",
    "- Create features, using window and aggreation functions in an Apache Beam pipeline\n",
    "- Deploy the Apache Beam pipeline to Dataflow\n",
    "- Ingest engineered features from Dataflow into Vertex AI Feature Store\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Pub/Sub](https://cloud.google.com/pubsub/)\n",
    "- [DataFlow](https://cloud.google.com/dataflow/)\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "\n",
    "Step performed in this notebook:\n",
    "\n",
    "- calculate customer spending features (last 15-mins, 30-mins, and 60-mins)\n",
    "- calculate terminal activity features (last 15-mins, 30-mins, and 60-mins)\n",
    "\n",
    "by pulling the streaming data from a Pub/Sub topic using the Pub/Sub subscription that we created in `00_environment_setup.ipynb` and ingesting the streaming features directly into Vertex AI Feature Store using Dataflow. \n",
    "\n",
    "![image](./misc/images/streaming-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "535c9218e18d"
   },
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "94fe7f01ef5a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUCKET_NAME          = \"firstproject-315903-fraudfinder\"\n",
      "PROJECT              = \"firstproject-315903\"\n",
      "REGION               = \"us-central1\"\n",
      "ID                   = \"yyg4u\"\n",
      "FEATURESTORE_ID      = \"fraudfinder_yyg4u\"\n",
      "MODEL_NAME           = \"ff_model\"\n",
      "ENDPOINT_NAME        = \"ff_model_endpoint\"\n",
      "TRAINING_DS_SIZE     = \"1000\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folder\n",
    "\n",
    "In favour of clean folder structure, we will create a separate folder and place all the files that we will produce there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER = \"./beam_pipeline\"\n",
    "PYTHON_SCRIPT = f\"{FOLDER}/main.py\"\n",
    "REQUIREMENTS_FILE = f\"{FOLDER}/requirements.txt\"\n",
    "\n",
    "# Create new folder for pipeline files\n",
    "!rm -rf {FOLDER} || True\n",
    "!mkdir {FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we begin\n",
    "\n",
    "As deploying Apache Beam pipelines to Dataflow works better if we submit the job from a Python script, we will be writting the code into a python script instead of running directly on the notebook. \n",
    "\n",
    "In the next cells, we write the cell contents to a Python script `main.py`. We are NOT running the code direcly and an additional invocation is required. The notebook is done this way for eaiser demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf",
    "tags": []
   },
   "source": [
    "### Write import statements\n",
    "\n",
    "Here we write the code to import all the required libraries to the external python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a725b800a52b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./beam_pipeline/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PYTHON_SCRIPT}\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from typing import Tuple, Any, List\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.transforms.combiners import CountCombineFn, MeanCombineFn\n",
    "    \n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bd54c684051"
   },
   "source": [
    "### Defining an auxiliary magic function\n",
    "\n",
    "The magic function `writefile` from Jupyter Notebook can only write the cell as is and could not unpack Python variables. Hence, we need to create an auxiliary magic function that can unpack Python variables and write them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ad8dba046d05",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, \"a\") as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a74807b98c34"
   },
   "source": [
    "### Write the variable values\n",
    "\n",
    "Here we write the variable values to the external python script using the new magic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3a4f9df411aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding additional variables to project_variables\n",
    "project_variables = \"\\n\".join(config[1:-1])\n",
    "project_variables += f'\\nPROJECT_ID = \"{PROJECT}\"'\n",
    "project_variables += f'\\nBUCKET_NAME = \"{BUCKET_NAME}\"'\n",
    "project_variables += f'\\nREQUIREMENTS_FILE = \"{REQUIREMENTS_FILE}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5055cc7ce79e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writetemplate {PYTHON_SCRIPT}\n",
    "\n",
    "# Project variables\n",
    "{project_variables}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c82e89dd7f2c"
   },
   "source": [
    "### Write constant variables\n",
    "\n",
    "Here we write constant variables to the external python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "a437ab07d805",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./beam_pipeline/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {PYTHON_SCRIPT}\n",
    "\n",
    "# Pub/Sub variables\n",
    "SUBSCRIPTION_NAME = \"ff-tx-for-feat-eng-sub\"\n",
    "SUBSCRIPTION_PATH = f\"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\"\n",
    "\n",
    "# Dataflow variables\n",
    "FIFTEEN_MIN_IN_SECS = 15 * 60\n",
    "THIRTY_MIN_IN_SECS = 30 * 60\n",
    "WINDOW_SIZE = 60 * 60 # 1 hour in secs\n",
    "WINDOW_PERIOD = 1 * 60  # 1 min in secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11185856b722"
   },
   "source": [
    "### Defining auxiliary functions and classes\n",
    "\n",
    "Here we define auxiliary functions and classes that will be used in building our real-time feature engineering and ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dabd10a0a18e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./beam_pipeline/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {PYTHON_SCRIPT}\n",
    "\n",
    "def to_unix_time(time_str: str, time_format='%Y-%m-%d %H:%M:%S') -> int:\n",
    "    \"\"\"\n",
    "    Convert a time string to Unix time\n",
    "    Args:\n",
    "        time_str: time string\n",
    "        time_format: time format\n",
    "    Returns:\n",
    "        unix_time: Unix time\n",
    "    \"\"\"\n",
    "    import time\n",
    "    # Converts a time string into Unix time\n",
    "    time_tuple = time.strptime(time_str, time_format)\n",
    "    return int(time.mktime(time_tuple))\n",
    "    \n",
    "    \n",
    "class AddAddtionalInfo(beam.DoFn):\n",
    "    \n",
    "    # Add composite key and difference from window end timestamp to element\n",
    "    def process(self, element: Tuple, timestamp=beam.DoFn.TimestampParam, window=beam.DoFn.WindowParam) -> Tuple:\n",
    "        \"\"\"\n",
    "        Add composite key and difference from window end timestamp to element\n",
    "        Args:\n",
    "            element: element to process\n",
    "            timestamp: timestamp of element\n",
    "            window: window of element\n",
    "        Returns:\n",
    "            element: element with composite key and difference from window end timestamp\n",
    "        \"\"\"\n",
    "        window_end_dt = window.end.to_utc_datetime().strftime(\"%Y%m%d%H%M%S\")\n",
    "        new_element = {\n",
    "            'TX_ID': element['TX_ID'],\n",
    "            'TX_TS': element['TX_TS'],\n",
    "            'CUSTOMER_ID': element['CUSTOMER_ID'],\n",
    "            'TERMINAL_ID': element['TERMINAL_ID'],\n",
    "            'TX_AMOUNT': element['TX_AMOUNT'],\n",
    "            'CUSTOMER_ID_COMPOSITE_KEY': f\"{element['CUSTOMER_ID']}_{window_end_dt}\",\n",
    "            'TERMINAL_ID_COMPOSITE_KEY': f\"{element['TERMINAL_ID']}_{window_end_dt}\",\n",
    "            'TS_DIFF': window.end - timestamp\n",
    "        }\n",
    "        return (new_element,)\n",
    "\n",
    "\n",
    "class WriteFeatures(beam.DoFn):\n",
    "    def __init__(self, resource_name: str):\n",
    "        self.resource_name = resource_name\n",
    "\n",
    "    def populate_customer_payload(self, new_records, aggregated) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Prepare payloads for customer related features to be written\n",
    "        at the Vertex AI Feature store. The values are required to be of FeatureValue type.\n",
    "        Args:\n",
    "            new_records: new records to write\n",
    "            aggregated: aggregated records to write\n",
    "        Returns:\n",
    "            payloads: list of payloads to write\n",
    "        \"\"\"\n",
    "        payloads = []\n",
    "        for row in new_records:\n",
    "            payload = aiplatform_v1beta1.WriteFeatureValuesPayload()\n",
    "            payload.entity_id = row.CUSTOMER_ID\n",
    "            payload.feature_values = {\n",
    "                \"customer_id_nb_tx_15min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    int64_value=aggregated.CUSTOMER_ID_NB_TX_15MIN_WINDOW),\n",
    "                \"customer_id_nb_tx_30min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    int64_value=aggregated.CUSTOMER_ID_NB_TX_30MIN_WINDOW),\n",
    "                \"customer_id_nb_tx_60min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    int64_value=aggregated.CUSTOMER_ID_NB_TX_60MIN_WINDOW),\n",
    "                \"customer_id_avg_amount_15min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    double_value=aggregated.CUSTOMER_ID_SUM_AMOUNT_15MIN_WINDOW / aggregated.CUSTOMER_ID_NB_TX_15MIN_WINDOW),\n",
    "                \"customer_id_avg_amount_30min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    double_value=aggregated.CUSTOMER_ID_SUM_AMOUNT_30MIN_WINDOW / aggregated.CUSTOMER_ID_NB_TX_30MIN_WINDOW),\n",
    "                \"customer_id_avg_amount_60min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    double_value=aggregated.CUSTOMER_ID_AVG_AMOUNT_60MIN_WINDOW),\n",
    "            }\n",
    "            payloads.append(payload)\n",
    "        return payloads\n",
    "\n",
    "    def populate_terminal_payload(self, new_records, aggregated) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Prepare payloads for terminal related features to be written\n",
    "        at the Vertex AI Feature store. The values are required to be of FeatureValue type.\n",
    "        Args:\n",
    "            new_records: new records to write\n",
    "            aggregated: aggregated records to write\n",
    "        Returns:\n",
    "            payloads: list of payloads to write\n",
    "        \"\"\"\n",
    "        payloads = []\n",
    "        for row in new_records:\n",
    "            payload = aiplatform_v1beta1.WriteFeatureValuesPayload()\n",
    "            payload.entity_id = row.TERMINAL_ID\n",
    "            payload.feature_values = {\n",
    "                \"terminal_id_nb_tx_15min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    int64_value=aggregated.TERMINAL_ID_NB_TX_15MIN_WINDOW),\n",
    "                \"terminal_id_nb_tx_30min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    int64_value=aggregated.TERMINAL_ID_NB_TX_30MIN_WINDOW),\n",
    "                \"terminal_id_nb_tx_60min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    int64_value=aggregated.TERMINAL_ID_NB_TX_60MIN_WINDOW),\n",
    "                \"terminal_id_avg_amount_15min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    double_value=aggregated.TERMINAL_ID_SUM_AMOUNT_15MIN_WINDOW / aggregated.TERMINAL_ID_NB_TX_15MIN_WINDOW),\n",
    "                \"terminal_id_avg_amount_30min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    double_value=aggregated.TERMINAL_ID_SUM_AMOUNT_30MIN_WINDOW / aggregated.TERMINAL_ID_NB_TX_30MIN_WINDOW),\n",
    "                \"terminal_id_avg_amount_60min_window\": aiplatform_v1beta1.FeatureValue(\n",
    "                    double_value=aggregated.TERMINAL_ID_AVG_AMOUNT_60MIN_WINDOW),\n",
    "            }\n",
    "            payloads.append(payload)\n",
    "        return payloads\n",
    "    \n",
    "    #TODO: Add response to docstring\n",
    "    def send_request_to_feature_store(self, resource_name: str, payloads: List[Any]):\n",
    "        \"\"\"\n",
    "        Sends a write request to vertex ai feature store by preparing \n",
    "        a write feature value request using provided resource name and payloads, and \n",
    "        by making use of a feature store online serving service client\n",
    "        Args:\n",
    "            resource_name: resource name of the feature store\n",
    "            payloads: list of payloads to write\n",
    "        \"\"\"\n",
    "        # Prepare request\n",
    "        request = aiplatform_v1beta1.WriteFeatureValuesRequest(\n",
    "            entity_type=resource_name,\n",
    "            payloads=payloads,\n",
    "        )\n",
    "\n",
    "        # Create feature store online serving service client\n",
    "        client_options = {\n",
    "            \"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"\n",
    "        }\n",
    "        v1beta1_client = aiplatform_v1beta1.FeaturestoreOnlineServingServiceClient(client_options=client_options)\n",
    "\n",
    "        # Send the request\n",
    "        response = v1beta1_client.write_feature_values(request=request)\n",
    "        return response\n",
    "\n",
    "    def process(self, element: Tuple) -> Tuple:\n",
    "        \"\"\"\n",
    "        Select entity using resource_name variable and \n",
    "        write the respective features to Vertex AI Feature store\n",
    "        Args:\n",
    "            element: tuple of new records and aggregated records\n",
    "        Returns:\n",
    "            element: tuple of new records and aggregated records\n",
    "        \"\"\"\n",
    "        new_records = element[1]['new_records']\n",
    "        aggregated = element[1]['aggregated'][0]\n",
    "\n",
    "        entity = self.resource_name.split(\"/\")[-1]\n",
    "        payloads = []\n",
    "        message = \"\"\n",
    "\n",
    "        if entity == \"customer\":\n",
    "            payloads = self.populate_customer_payload(new_records, aggregated)\n",
    "            customer_ids = [x.entity_id for x in payloads]\n",
    "            message = f\"Inserted features for CUSTOMER IDs: {', '.join(customer_ids)}\"\n",
    "\n",
    "        elif entity == \"terminal\":\n",
    "            payloads = self.populate_terminal_payload(new_records, aggregated)\n",
    "            terminal_ids = [x.entity_id for x in payloads]\n",
    "            message = f\"Inserted features for TERMINAL IDs: {', '.join(terminal_ids)}\"\n",
    "\n",
    "        response = self.send_request_to_feature_store(self.resource_name, payloads)\n",
    "        logging.info(message)\n",
    "\n",
    "        yield (response,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ee7d34ce9b1"
   },
   "source": [
    "### Building the pipeline\n",
    "\n",
    "Now we are ready to build the pipeline using the defined classes and functions above. Once the pipeline is ready, we will wrap everything into a main function and submit it to the Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1c77896262f8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./beam_pipeline/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {PYTHON_SCRIPT}\n",
    "\n",
    "def main():\n",
    "    # Initialize Vertex AI client\n",
    "    aiplatform.init(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION\n",
    "    )\n",
    "\n",
    "    # Get entity types for customer and terminal\n",
    "    fs = aiplatform.featurestore.Featurestore(\n",
    "        featurestore_name=FEATURESTORE_ID\n",
    "    )\n",
    "    customer_entity_type = fs.get_entity_type(\"customer\")\n",
    "    terminal_entity_type = fs.get_entity_type(\"terminal\")\n",
    "    \n",
    "    # Setup pipeline options for deploying to dataflow\n",
    "    pipeline_options = PipelineOptions(streaming=True, \n",
    "                                       save_main_session=True,\n",
    "                                       runner=\"DataflowRunner\",\n",
    "                                       project=PROJECT_ID,\n",
    "                                       region=REGION,\n",
    "                                       temp_location=f\"gs://{BUCKET_NAME}/dataflow/tmp\",\n",
    "                                       requirements_file=REQUIREMENTS_FILE,\n",
    "                                       max_num_workers=2)\n",
    "    \n",
    "    # Build pipeline and transformation steps\n",
    "    pipeline = beam.Pipeline(options=pipeline_options)\n",
    "    \n",
    "    source = (\n",
    "        pipeline\n",
    "        | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(subscription=SUBSCRIPTION_PATH)\n",
    "        | 'Decode byte array to json dict' >> beam.Map(lambda row: json.loads(row.decode('utf-8')))\n",
    "    )\n",
    "\n",
    "    enriched_source = (\n",
    "        source\n",
    "        | 'Attach timestamps' >> beam.Map(lambda row: beam.window.TimestampedValue(row, to_unix_time(row['TX_TS'])))\n",
    "        | 'Create sliding window' >> beam.WindowInto(beam.window.SlidingWindows(WINDOW_SIZE, WINDOW_PERIOD, offset=WINDOW_SIZE))\n",
    "        | 'Add window info' >> beam.ParDo(AddAddtionalInfo())\n",
    "        | 'Convert to namedtuple' >> beam.Map(lambda row: beam.Row(**row))\n",
    "    )\n",
    "\n",
    "    new_records = (\n",
    "        enriched_source\n",
    "        | 'Filter only new rows' >> beam.Filter(lambda row: row.TS_DIFF <= WINDOW_PERIOD)\n",
    "    )\n",
    "\n",
    "    # Build customer features\n",
    "    new_records_customer_id = (\n",
    "        new_records\n",
    "        | 'Assign CUSTOMER_ID_COMPOSITE_KEY as key' >> beam.WithKeys(lambda row: row.CUSTOMER_ID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    aggregated_customer_id = (\n",
    "        enriched_source\n",
    "        | 'Group by customer id composite key column' >> beam.GroupBy(CUSTOMER_ID_COMPOSITE_KEY='CUSTOMER_ID_COMPOSITE_KEY')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= FIFTEEN_MIN_IN_SECS else 0, sum, 'CUSTOMER_ID_NB_TX_15MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= THIRTY_MIN_IN_SECS else 0, sum, 'CUSTOMER_ID_NB_TX_30MIN_WINDOW')\n",
    "            .aggregate_field('TX_ID', CountCombineFn(), 'CUSTOMER_ID_NB_TX_60MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= FIFTEEN_MIN_IN_SECS else 0, sum,'CUSTOMER_ID_SUM_AMOUNT_15MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= THIRTY_MIN_IN_SECS else 0, sum, 'CUSTOMER_ID_SUM_AMOUNT_30MIN_WINDOW')\n",
    "            .aggregate_field('TX_AMOUNT', MeanCombineFn(), 'CUSTOMER_ID_AVG_AMOUNT_60MIN_WINDOW')\n",
    "        | 'Assign key for aggregated results (customer id)' >> beam.WithKeys(lambda row: row.CUSTOMER_ID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    merged_customer_id = (\n",
    "        ({\n",
    "            'new_records': new_records_customer_id, \n",
    "            'aggregated': aggregated_customer_id\n",
    "        })\n",
    "        | 'Merge pcollections (customer id)' >> beam.CoGroupByKey()\n",
    "        | 'Filter empty rows (customer id)' >> beam.Filter(lambda row: len(row[1]['new_records']) > 0)\n",
    "        | 'Write to feature store (customer id)' >> beam.ParDo(WriteFeatures(customer_entity_type.resource_name))\n",
    "    )\n",
    "\n",
    "    # Build terminal features\n",
    "    new_records_terminal_id = (\n",
    "        new_records\n",
    "        | 'Assign TERMINAL_ID_COMPOSITE_KEY as key' >> beam.WithKeys(lambda row: row.TERMINAL_ID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    aggregated_terminal_id = (\n",
    "        enriched_source\n",
    "        | 'Group by terminal id composite key column' >> beam.GroupBy(TERMINAL_ID_COMPOSITE_KEY='TERMINAL_ID_COMPOSITE_KEY')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= FIFTEEN_MIN_IN_SECS else 0, sum, 'TERMINAL_ID_NB_TX_15MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= THIRTY_MIN_IN_SECS else 0, sum, 'TERMINAL_ID_NB_TX_30MIN_WINDOW')\n",
    "            .aggregate_field('TX_ID', CountCombineFn(), 'TERMINAL_ID_NB_TX_60MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= FIFTEEN_MIN_IN_SECS else 0, sum, 'TERMINAL_ID_SUM_AMOUNT_15MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= THIRTY_MIN_IN_SECS else 0, sum, 'TERMINAL_ID_SUM_AMOUNT_30MIN_WINDOW')\n",
    "            .aggregate_field('TX_AMOUNT', MeanCombineFn(), 'TERMINAL_ID_AVG_AMOUNT_60MIN_WINDOW')\n",
    "        | 'Assign key for aggregated results (terminal id)' >> beam.WithKeys(lambda row: row.TERMINAL_ID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    merged_terminal_id = (\n",
    "        ({\n",
    "            'new_records': new_records_terminal_id, \n",
    "            'aggregated': aggregated_terminal_id\n",
    "        })\n",
    "        | 'Merge pcollections (terminal id)' >> beam.CoGroupByKey()\n",
    "        | 'Filter empty rows (terminal id)' >> beam.Filter(lambda row: len(row[1]['new_records']) > 0)\n",
    "        | 'Write to feature store (terminal id)' >> beam.ParDo(WriteFeatures(terminal_entity_type.resource_name))\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline (async)\n",
    "    pipeline.run()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9133e7c54aa"
   },
   "source": [
    "### Creating `requirement.txt` for Dataflow Workers\n",
    "\n",
    "As we are using `google-cloud-aiplatform` and `google-apitools` package, we need to pass the `requirement.txt` to the Dataflow Workers so that the workers will install the packages in their respective environment before running the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0b58205547e1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./beam_pipeline/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REQUIREMENTS_FILE}\n",
    "\n",
    "google-cloud-aiplatform\n",
    "google-apitools==0.5.32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9ede8f5aff0"
   },
   "source": [
    "### Deploying the pipeline\n",
    "\n",
    "Now we are ready to deploy this pipeline to Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-beam[gcp] in /opt/conda/lib/python3.10/site-packages (2.52.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.7)\n",
      "Requirement already satisfied: orjson<4,>=3.9.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (3.9.10)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.3.1.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.9.1)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.19)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.59.2)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.7.3)\n",
      "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.22.0)\n",
      "Requirement already satisfied: js2py<1,>=0.74 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.74)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (4.19.2)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.24.4)\n",
      "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.6.1)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (23.2)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (4.6.1)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (3.20.3)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2023.3.post1)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2023.10.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (4.8.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.22.0)\n",
      "Requirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix<1 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.6)\n",
      "Requirement already satisfied: cachetools<6,>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (5.3.2)\n",
      "Collecting google-api-core<3,>=2.0.0 (from apache-beam[gcp])\n",
      "  Downloading google_api_core-2.15.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-apitools<0.5.32,>=0.5.31 (from apache-beam[gcp])\n",
      "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.23.4)\n",
      "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.1.1)\n",
      "Collecting google-cloud-datastore<3,>=2.0.0 (from apache-beam[gcp])\n",
      "  Downloading google_cloud_datastore-2.18.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.18.4)\n",
      "Collecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp])\n",
      "  Downloading google_cloud_pubsublite-1.8.3-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (3.13.0)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.3.3)\n",
      "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp])\n",
      "  Downloading google_cloud_bigtable-2.21.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp])\n",
      "  Downloading google_cloud_spanner-3.40.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp])\n",
      "  Downloading google_cloud_dlp-3.14.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: google-cloud-language<3,>=2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.11.1)\n",
      "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp])\n",
      "  Downloading google_cloud_videointelligence-2.12.0-py2.py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-cloud-vision<4,>=2 (from apache-beam[gcp])\n",
      "  Downloading google_cloud_vision-3.5.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp])\n",
      "  Downloading google_cloud_recommendations_ai-0.10.6-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.36.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3,>=2.0.0->apache-beam[gcp]) (1.61.0)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /opt/conda/lib/python3.10/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (4.1.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (4.9)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (2.13.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (1.10.4)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (2.0.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]) (2.6.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]) (0.12.6)\n",
      "Requirement already satisfied: grpcio-status>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (1.48.2)\n",
      "Collecting overrides<7.0.0,>=6.0.1 (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp])\n",
      "  Downloading overrides-6.5.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: sqlparse>=0.4.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]) (0.4.4)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]) (3.1.1)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /opt/conda/lib/python3.10/site-packages (from js2py<1,>=0.74->apache-beam[gcp]) (5.2)\n",
      "Requirement already satisfied: pyjsparser>=2.5.1 in /opt/conda/lib/python3.10/site-packages (from js2py<1,>=0.74->apache-beam[gcp]) (2.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (0.12.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]) (2.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2023.7.22)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (1.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (0.5.0)\n",
      "Downloading google_api_core-2.15.0-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.0/122.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_bigtable-2.21.0-py2.py3-none-any.whl (293 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.0/293.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_datastore-2.18.0-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.3/177.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_dlp-3.14.0-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.5/157.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_pubsublite-1.8.3-py2.py3-none-any.whl (288 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_recommendations_ai-0.10.6-py2.py3-none-any.whl (172 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_spanner-3.40.1-py2.py3-none-any.whl (332 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.9/332.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_videointelligence-2.12.0-py2.py3-none-any.whl (228 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.6/228.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_vision-3.5.0-py2.py3-none-any.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: google-apitools\n",
      "  Building wheel for google-apitools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131016 sha256=6e6481f69a15958a31fab966deab0326b3b658e10b8c693ca7392bd86297d4cb\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/04/b7/e0/9712f8c23a5da3d9d16fb88216b897bf60e85b12f5470f26ee\n",
      "Successfully built google-apitools\n",
      "Installing collected packages: overrides, google-apitools, google-api-core, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-pubsublite\n",
      "  Attempting uninstall: overrides\n",
      "    Found existing installation: overrides 7.4.0\n",
      "    Uninstalling overrides-7.4.0:\n",
      "      Successfully uninstalled overrides-7.4.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.34.0\n",
      "    Uninstalling google-api-core-1.34.0:\n",
      "      Successfully uninstalled google-api-core-1.34.0\n",
      "  Attempting uninstall: google-cloud-datastore\n",
      "    Found existing installation: google-cloud-datastore 1.15.5\n",
      "    Uninstalling google-cloud-datastore-1.15.5:\n",
      "      Successfully uninstalled google-cloud-datastore-1.15.5\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/google/cloud/~atastore'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/google/cloud/~atastore_v1'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-2.15.0 google-apitools-0.5.31 google-cloud-bigtable-2.21.0 google-cloud-datastore-2.18.0 google-cloud-dlp-3.14.0 google-cloud-pubsublite-1.8.3 google-cloud-recommendations-ai-0.10.6 google-cloud-spanner-3.40.1 google-cloud-videointelligence-2.12.0 google-cloud-vision-3.5.0 overrides-6.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install 'apache-beam[gcp]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (from -r ./beam_pipeline/requirements.txt (line 2)) (1.36.1)\n",
      "Collecting google-apitools==0.5.32 (from -r ./beam_pipeline/requirements.txt (line 3))\n",
      "  Downloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httplib2>=0.8 in /opt/conda/lib/python3.10/site-packages (from google-apitools==0.5.32->-r ./beam_pipeline/requirements.txt (line 3)) (0.22.0)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.10/site-packages (from google-apitools==0.5.32->-r ./beam_pipeline/requirements.txt (line 3)) (0.19)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /opt/conda/lib/python3.10/site-packages (from google-apitools==0.5.32->-r ./beam_pipeline/requirements.txt (line 3)) (4.1.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from google-apitools==0.5.32->-r ./beam_pipeline/requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (2.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (2.13.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (3.13.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (1.10.4)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (1.61.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (2.23.4)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (1.59.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (1.48.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (0.12.6)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2>=0.8->google-apitools==0.5.32->-r ./beam_pipeline/requirements.txt (line 3)) (3.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=1.4.12->google-apitools==0.5.32->-r ./beam_pipeline/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=1.4.12->google-apitools==0.5.32->-r ./beam_pipeline/requirements.txt (line 3)) (0.3.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=1.4.12->google-apitools==0.5.32->-r ./beam_pipeline/requirements.txt (line 3)) (4.9)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (1.24.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (5.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r ./beam_pipeline/requirements.txt (line 2)) (2023.7.22)\n",
      "Installing collected packages: google-apitools\n",
      "  Attempting uninstall: google-apitools\n",
      "    Found existing installation: google-apitools 0.5.31\n",
      "    Uninstalling google-apitools-0.5.31:\n",
      "      Successfully uninstalled google-apitools-0.5.31\n",
      "Successfully installed google-apitools-0.5.32\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./beam_pipeline/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0601ef9a1b20",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py\", line 79, in error_remapped_callable\n",
      "    return callable_(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/grpc/_channel.py\", line 1161, in __call__\n",
      "    return _end_unary_response_blocking(state, call, False, None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/grpc/_channel.py\", line 1004, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.NOT_FOUND\n",
      "\tdetails = \"The Featurestore does not exist.\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:209.85.200.95:443 {grpc_message:\"The Featurestore does not exist.\", grpc_status:5, created_time:\"2023-12-10T07:28:45.573860808+00:00\"}\"\n",
      ">\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/fraudfinder/./beam_pipeline/main.py\", line 311, in <module>\n",
      "    main()\n",
      "  File \"/home/jupyter/fraudfinder/./beam_pipeline/main.py\", line 212, in main\n",
      "    fs = aiplatform.featurestore.Featurestore(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/featurestore/featurestore.py\", line 108, in __init__\n",
      "    self._gca_resource = self._get_gca_resource(resource_name=featurestore_name)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py\", line 646, in _get_gca_resource\n",
      "    return getattr(self.api_client, self._getter_method)(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/featurestore_service/client.py\", line 759, in get_featurestore\n",
      "    response = rpc(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/api_core/retry.py\", line 372, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/api_core/retry.py\", line 207, in retry_target\n",
      "    result = target()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py\", line 81, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.NotFound: 404 The Featurestore does not exist.\n"
     ]
    }
   ],
   "source": [
    "!python3 {PYTHON_SCRIPT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! Now the job should be running on <a href=\"https://console.cloud.google.com/dataflow/jobs\">Dataflow<a>!\n",
    "    \n",
    "If everything went well, you should see this Dataflow pipeline diagram on Dataflow Console.\n",
    "    \n",
    "![image](./misc/images/streaming-dataflow-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying the ingestion pipeline\n",
    "\n",
    "Once the dataflow pipeline is up and running, you should be able to see which feature entities are being ingested via the `Step Log` of respective `Write to feature store` step. \n",
    "\n",
    "To verify whether the data ingestion job is doing what it is supposed to be doing, copy a list of entity ids from the logs and use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "fs = vertex_ai.featurestore.Featurestore(featurestore_name=FEATURESTORE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_entity_ids = [\"5830444124423549\", \"5469689693941771\", \"1361459972478769\"]  # copy customer ids from Write to feature store (customer id) step's log\n",
    "\n",
    "customer_entity_type = fs.get_entity_type(\"customer\")\n",
    "customer_aggregated_features = customer_entity_type.read(\n",
    "    entity_ids=customer_entity_ids\n",
    ")\n",
    "\n",
    "customer_aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_entity_ids = [\"97802258\", \"48770968\", \"98391079\"] # copy terminal ids from Write to feature store (terminal id) step's log\n",
    "\n",
    "terminal_entity_type = fs.get_entity_type(\"terminal\")\n",
    "terminal_aggregated_features = terminal_entity_type.read(\n",
    "    entity_ids=terminal_entity_ids\n",
    ")\n",
    "\n",
    "terminal_aggregated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END\n",
    "\n",
    "If you want to explore BigQuery ML pipeline, you can go here: `bqml/04_model_training_and_prediction.ipynb`\n",
    "\n",
    "Or else, if you want to explore Vertex ML pipeline, go here: `vertex_ai/04_experimentation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_feature_engineering_streaming.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
